[scraping]

# Boards to scrape
boards = [""]

# Seconds to wait between fetching the threads.json of each board
poll_interval = 60

# On startup, fetch (and potentially update) threads from archive.json
fetch_archive = true

# Download full image/media files and/or thumbnails
download_media = true
download_thumbs = true


[rate_limiting]
# `interval` is in seconds.
# `max_interval` is the maximum number of requests that can be made in an interval.
# `max_concurrent` is the maximum number of requests that can run at once.

# Media and image files
media = { interval = 60, max_interval = 600, max_concurrent = 150 }
# Threads
thread = { interval = 60, max_interval = 200, max_concurrent = 75 }
# threads.json and archive.json
thread_list = { interval = 60, max_interval = 180, max_concurrent = 50 }


[database_media]
database_url = "mysql://username:password@localhost/ena"
charset = "utf8mb4"
media_dir = "media"


[asagi_compat]

# Adjust UTC timestamps to "America/New_York" (should be `true` for compatibility)
adjust_timestamps = true

# On archived boards, fetch threads after they're bumped off. At the cost of an extra request, this
# allows for more accurate archive times (how much more depends on `poll_interval`) and catches sage
# posts/deletions/changes which would otherwise have been missed (should be `true` for compatibility)
refetch_archived_threads = true

# Add archive times to bumped-off threads on boards without archiving (should be `false` for
# compatibility)
always_add_archive_times = false

# Create the `index_counters` table used by Sphinx/FoolFuuka (should be `true` for compatibility)
create_index_counters = true
